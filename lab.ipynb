{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "867ca101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DATA_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "def read_df(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Reads a given file from the given path,\n",
    "        Determine the data type from the name,\n",
    "        returns a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if data_path.endswith('.csv'):\n",
    "            return pd.read_csv(data_path)\n",
    "        elif data_path.endswith('.xlsx') or data_path.endswith('.xls'):\n",
    "            return pd.read_excel(data_path)\n",
    "        elif data_path.endswith('.json'):\n",
    "            return pd.read_json(data_path, lines=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a CSV, Excel, or JSON file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08658fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted roaming data with 200 records.\n",
      "Extracted usage data with 500 records.\n",
      "Extracted sessions data with 300 records.\n"
     ]
    }
   ],
   "source": [
    "paths_to_data = [\n",
    "    \"partner_roaming.xlsx\",\n",
    "    \"raw_usage_2025_01.csv\",\n",
    "    \"sessions.json\"\n",
    "]\n",
    "\n",
    "def extract_all_data(file_paths): # Note: file_paths is a list of file paths and only csv, json, and excel files are supported\n",
    "    if not file_paths or len(file_paths) == 0:\n",
    "        raise ValueError(\"The list of file paths is empty.\")\n",
    "    data_frames = {}\n",
    "    for path in file_paths:\n",
    "        df = read_df(f\"{RAW_DATA_DIR}/{path}\")\n",
    "        df_key = Path(path).stem.split('/')[-1]  # Use the file name without extension\n",
    "        # get the key word roaming, usage, sessions as the key for the file that contains the word and assign it as the key.\n",
    "        df_key = 'roaming' if 'roaming' in df_key else 'usage' if 'usage' in df_key else 'sessions' if 'sessions' in df_key else df_key\n",
    "        data_frames[df_key] = df\n",
    "        print(f\"Extracted {df_key} data with {len(df)} records.\")\n",
    "    return data_frames\n",
    "\n",
    "# call the function to extract data\n",
    "all_data_frames = extract_all_data(paths_to_data)\n",
    "data_frames = all_data_frames.copy()\n",
    "#print(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e952456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msisdn', 'country', 'roaming_data_mb', 'roaming_cost_usd', 'date'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frames['roaming'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2c543ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dfs(dfs):\n",
    "    cleaned = {}\n",
    "    for key, df in dfs.items():\n",
    "        if df.empty:\n",
    "            cleaned[key] = df\n",
    "            continue  # Skip empty DataFrames\n",
    "        \n",
    "        # Make a copy to avoid modifying the original\n",
    "        df = df.copy()\n",
    "        \n",
    "        df.columns = df.columns.str.lower().str.strip().str.replace(\" \", \"_\", regex=False) #format the columns to lower_case and remove space\n",
    "\n",
    "        if 'download_mb' in df.columns and 'upload_mb' in df.columns:\n",
    "            df = df.rename(columns={\n",
    "                'download_mb' : 'Download MB',\n",
    "                'upload_mb' : 'Upload MB',\n",
    "            })\n",
    "        \n",
    "        if 'avg_throughput' in df.columns:\n",
    "            median_throughput = df['avg_throughput'].median()   #calculate median of avg_throughput\n",
    "            df['avg_throughput'] = df['avg_throughput'].fillna(median_throughput)\n",
    "            \n",
    "        if 'session_id' in df.columns:    \n",
    "            df = df.drop_duplicates(subset=['session_id'], keep='first') \n",
    "        \n",
    "        if 'Download MB' in df.columns and 'Upload MB' in df.columns:\n",
    "            df['total_usage_mb'] = df['Download MB'].fillna(0) + df['Upload MB'].fillna(0)\n",
    "\n",
    "        # drop -ve values for duration_ms\n",
    "        if 'duration_ms' in df.columns:\n",
    "            df = df[df['duration_ms'] >= 0]\n",
    "            \n",
    "        #parse date columns to datetime format if any\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "            \n",
    "        #replace missing values in 'app_category' with 'unknown'\n",
    "        if 'app_category' in df.columns:\n",
    "            df['app_category'] = df['app_category'].fillna('unknown')\n",
    "        \n",
    "        cleaned[key] = df\n",
    "            \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b29eca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dfs = clean_dfs(data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "835b9e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msisdn', 'session_id', 'timestamp', 'Download MB', 'Upload MB',\n",
       "       'avg_throughput', 'latency_ms', 'duration_ms', 'region',\n",
       "       'total_usage_mb'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dfs['usage'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197436a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea66275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_daily_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    if df.empty or 'msisdn' not in df.columns or 'timestamp' not in df.columns:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if its invalid\n",
    "    #GroupBy\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    agg = (\n",
    "        df.groupby(['msisdn', 'date'])\n",
    "        .agg(\n",
    "            total_usage_mb = ('total_usage_mb', 'sum'),\n",
    "            avg_throughput = ('avg_throughput', 'mean'),\n",
    "            sessions = ('session_id', 'nunique'),\n",
    "            latensy_ms = ('latency_ms', 'mean') if 'latency_ms' in df.columns else ('timestamp', 'count')\n",
    "        ).reset_index()\n",
    "    )\n",
    "    \n",
    "    agg['total_usage_mb'] = agg['total_usage_mb'].astype(float)\n",
    "    agg['sessions'] = agg['sessions'].astype(int)\n",
    "    agg['avg_throughput'] = agg['avg_throughput'].astype(float)\n",
    "    return agg\n",
    "\n",
    "daily_usage_agg = aggregate_daily_usage(cleaned_dfs['usage'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33b879f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "    'cleaned_data': cleaned_dfs,\n",
    "    'daily_usage_aggregation': daily_usage_agg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c2fa766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_connection():\n",
    "    \"\"\"Create a database connection to the PostgreSQL database.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",\n",
    "            database=\"skylinkDB\",\n",
    "            user=\"postgres\",\n",
    "            password=\"@PGAdmin2025\",\n",
    "            port=\"5433\"\n",
    "        )\n",
    "        print(\"Connection successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return conn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03a47d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to execute SQL queries\n",
    "def execute_query(connection, query):\n",
    "    if connection:\n",
    "        cur = connection.cursor()\n",
    "    else:\n",
    "        cur = None\n",
    "    if cur:\n",
    "        try:\n",
    "            cur.execute(query)\n",
    "            connection.commit()\n",
    "            print(\"Query executed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            connection.rollback()\n",
    "        finally:\n",
    "            cur.close()\n",
    "            connection.close()\n",
    "    else:\n",
    "        print(\"No database connection available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d9ef91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['msisdn', 'session_id', 'timestamp', 'Download MB', 'Upload MB',\n",
       "       'avg_throughput', 'latency_ms', 'duration_ms', 'region',\n",
       "       'total_usage_mb', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data['cleaned_data']['usage'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n",
      "Query executed successfully\n"
     ]
    }
   ],
   "source": [
    "CREATE_TABLE_USAGE = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS USAGE (\n",
    "        msisdn VARCHAR(15) NOT NULL,\n",
    "        session_id INT,\n",
    "        timestamp DATE NOT NULL,\n",
    "        download_mb FLOAT,\n",
    "        upload_mb FLOAT,\n",
    "        avg_throughput FLOAT,\n",
    "        latency_ms FLOAT,\n",
    "        duration_ms INT,\n",
    "        region VARCHAR(50),\n",
    "    );\n",
    "\"\"\"\n",
    "conn = make_db_connection()\n",
    "\n",
    "execute_query(conn, CREATE_TABLE_USAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT_INTO_USAGE = \"\"\"\n",
    "    INSERT INTO USAGE (msisdn, date, total_usage_mb, avg_throughput, sessions, latensy_ms)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
